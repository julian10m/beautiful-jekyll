---
layout: post
title: Logistic and Softmax Regression
---

Logistic and Softmax regression are supervised machine learning algorithms used to estimate the probability of an event, usually the probability a posteriori of $y$ given $x$, where $y$ is a qualitative or categorical target variable that may take $K$ possible values and $x$ is an observation composed of $n$ features/predictors, i.e., $x=(x_1,\ldots,x_n)$. In general, these algorithms are combined with a decision rule, and thus used for classification tasks.


## Logistic Regression

This method is used for binary classification, i.e., cases where $K=2$. However, by combining multiple logistic regression classifiers, it is possible to extend their use to multinomial cases, i.e., scenarios where $K>2$.  

### Binary Classification

In these cases the target variable can only take two possibles values, e.g., "yes/no", "green/blue", "win/lose", etc. Hence, $y$ is usually modeled as a variable that can take either 0 or  1 as values, i.e., $y \in {0, 1}$.  This way, $y=1$ may represent "yes",  "green" and "win", while $y=0$ the opposite/remaining labels. 

To classify any sample $x$, we are interested in estimating $P(y/x)$. Indeed,  it can be proved that, to minimize the classification error,  $x$ needs to be assigned to the class that maximizes this probability.  In other words, if $P(y=1/x) > P(y=0/x)$, our guess $\hat{y}$ for $x$ would be $\hat{y} = 1$, and $\hat{y} = 0$ in the opposite case. In particular, logistic regression proposes 

$$\hat{P}(y/x) = \sigma(w^\intercal x + b)$$

where:

- $\hat{P}(y/x)$ is the value estimated for $P(y/x)$;
 - $w = (w_1, \ldots, w_n)$ is a weighting vector of $n$ parameters;
 - $b$ is a called a bias or intercept, and;
 - $\sigma(z) = \frac{1}{1+e^{-z}}$ is the sigmoid function.

The sigmoid function may take any real number as input, and will always output a value between 0 and 1.  In particular,  for large negative values of $z$, the term $e^{-z}$ becomes a large positive number, and thus $\sigma(z)$ tends to $0$.  On the other hand, when $z$ takes large positive values,  $e^{-z}$ approaches $0$, and thus $\sigma(z)$ is close to $1$. For $z=0$, since $e^0 = 1$, then $\sigma(0) = 1/2$. In addition, note that

$$1 - \sigma(z) = 1 -\frac{1}{1+e^{-z}}  = \frac{e^{-z}}{1+e^{-z}}$$

In particular, this means that

$$\log\Bigg(\frac{\sigma(z)}{1 - \sigma(z)}\Bigg) = \log\Big(\frac{1}{e^{-z}}\Big) = z$$

Since $\hat{P}(y/x) = \sigma(z)~\|~z = w^\intercal x + b$, then replacing we have

$$\log\Bigg(\frac{\hat{P}(y/x)}{1 - \hat{P}(y/x)}\Bigg) = w^\intercal x + b$$

where the term $\log(\frac{\hat{P}(y/x)}{1 - \hat{P}(y/x)})$ is known as the log-odds or logit of $y/x$. 

The last equation resembles the one we previously saw for linear regression, where $\hat{y} =  w^\intercal x + b$.  This gives us a hint of from where the logistic regression name comes: comparing both expressions,
it is straightforward to conclude that logistic regression is actually like applying linear regression to the logits of $y/x$.  

Despite the close relationship between linear and logistic regression, the first is used for regression tasks, but the latter in classification problems.  Indeed, logistic regressions additionally uses the following  classification rule

$$\hat{y} =  
\begin{cases}
    1,& \text{if } \hat{P}(y/x) = \sigma(w^\intercal x + b) \geq 0.5\\
    0,& \text{otherwise}
\end{cases}
 $$

Since $\sigma(z) = 0.5 \iff z = 0$, it is trivial to see that we can re-write this condition as 

 $$\hat{y} =  
\begin{cases}
    1,& \text{if } w^\intercal x + b \geq 0\\
    0,& \text{otherwise}
\end{cases}
 $$

Analyzing the previous expression, we can see that there exists a decision boundary in $w^\intercal x + b = 0$, i.e., once $w$ and $b$ are fixed, if $x$ is such that the computation of $w^\intercal x + b$ is greater than $0$, then $\hat{y} = 1$, and  $\hat{y} = 0$ otherwise. In particular, the values of $x$ that satisfy the condition $w^\intercal x + b = 0$ stand on an (affine) hyperplane in the sub-space generated by $(x_1, \ldots, x_n)$, where $b$ is the quantity by which this hyperplane is shifted from the origin. For a sample $x$ we want to classify, $\hat{y} = 1$ if $x$ falls on a specific side of this hyperplane, and $\hat{y} = 0$ in case it falls on the opposite side.

The optimal values for $w$ and $b$ can be estimated relying on the maximum likelihood estimation method. Given a training set of $m$ samples, without diving into the details, the following cost function needs to be minimized relying on the gradient descent algorithm

$$J(w, b) = -\frac{1}{m}\sum_{i = 1}^m \Bigg(y^{(i)} \log\Big(\hat{P}(y/x^{(i)})\Big)+ (1 - y^{(i)}) \log\Big(1 - \hat{P}(y/x^{(i)}\Big)\Bigg)$$

where $\hat{P}(y/x^{(i)})$ is the estimated probability that the sample $x^{(i)}$ might belong to class $y=1$ and $y^{(i)}$ is the class to which $x^{(i)}$ actually belongs to. Even though this formula looks intricate, it can be interpreted as simply averaging the result of a function $cost\Big(\hat{P}(y/x^{(i)}),  y^{(i)}\Big)$ for the $m$ samples, where

$$cost\Big(\hat{P}(y/x^{(i)}), y^{(i)}\Big) = 
\begin{cases}
    - \log(\hat{P}(y/x^{(i)}),& \text{if } y = 1\\
    - \log(1 - \hat{P}(y/x^{(i)})),& \text{otherwise}
\end{cases}
 $$

The intuition behind the expression of $J(w, b)$ is that we want to penalize the values of $w$ and $b$ not only for the mistakes that our classifier would introduce relying on them, but more generally for making doubtful classifications. In other words, for all samples $x^{(i)}$ and $x^{(j)}$ for which $y^{(i)}=1$ and $y^{(j)}=0$, we would want our classifier not to hesitate, i.e.,  to end up computing $\hat{P}(y =1 /x^{(i)}) = 1$ and $\hat{P}(y =0 /x^{(j)}) = 1$, respectively. To try to force this, what we do is to increase $J(w, b)$ when we find samples where the classifier was not completely convinced of its choice. For $y^{(i)}$ and $y^{(j)}$ we increase $J(w, b)$ in an amount $\log\big( \hat{P}(y =1 /x^{(i)})\big)$ and $\log\big(1 - \hat{P}(y =1 /x^{(j)})\big) = \log\big(\hat{P}(y =0 /x^{(j)})\big)$. Another possibility would have been to increase in $J(w, b)$ proportionally to the distance between the ideal and estimated values, i.e., $1 - \hat{P}(y =1 /x^{(i)})$ and $1 -\hat{P}(y =0 /x^{(j)})$. However, the log-scale used in logistic regression allows to penalize more worse decisions, e.g., for $y^{(i)} = 1$, then $\log\big(\hat{P}(y =1 /x^{(i)})\big)$ will be proportionally much larger the smaller $\hat{P}(y =1 /x^{(i)})$ is. During the training, the values of $w$ and $b$ will be iteratively updated, hoping that at the end of the process, the hyperplane separating zeros from ones will be reasonably well located, i.e.,  for most training samples, the estimations and classifications the algorithm will be good enough.

###  Multinomial Classification

When $K>2$, multiple logistic regression classifiers need to be used. In particular, two different approaches exist to combine them: one-vs-one (OvO) or one-vs-the-rest (OvR), also called one-vs-all (OvA).