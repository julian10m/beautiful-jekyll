---
layout: post
title: Logistic and Softmax Regression
---

Logistic and Softmax regression are supervised machine learning algorithms used to estimate the probability of an event, usually the probability a posteriori of $y$ given $x$, where $y$ is a qualitative or categorical target variable that may take $K$ possible values and $x$ is an observation composed of $n$ features/predictors, i.e., $x=(x_1,\ldots,x_n)$. In general, these algorithms are combined with a decision rule, and thus used for classification tasks.


## Logistic Regression

In particular, this method is used for binary classification, i.e., cases where $K=2$. However, it is possible to extend their use to cases where $K>2$ by combining multiple classifiers of this type in a one-vs-one (OvO) or one-vs-the-rest (OvR), also called one-vs-all (OvA), fashion.

### Binary Classification

In these cases $y \in {0, 1}$. We are interested in modeling $P(y/x)$ since assigning $x$ to the class that maximizes this probability actually reduces the classification error. In particular, logistic regression proposes that 

$$\hat{P}(y/x) = \sigma(w^\intercal x + b)$$

where:

 - $w = (w_1, \ldots, w_n)$ is a weighting vector of $n$ parameters;
 - $b$ is a called a bias or intercept, and;
 - $\sigma(z) = \frac{1}{1+e^{-z}}$ is the sigmoid function.

Note that

$$1 - \sigma(z) = 1 -\frac{1}{1+e^{-z}}  = \frac{e^{-z}}{1+e^{-z}}$$

Hence

$$log\Bigg(\frac{\sigma(z)}{1 - \sigma(z)}\Bigg) = log\Big(\frac{1}{e^{-z}}\Big) = z$$

Since $\hat{P}(y/x) = \sigma(z)~\|~z = w^\intercal x + b$, then replacing we have

$$log\Bigg(\frac{\hat{P}(y/x)}{1 - \hat{P}(y/x)}\Bigg) = w^\intercal x + b$$

Therefore, until here, logistic regression is like applying linear regression, where $y$ would be the left term on the last equation, that is known as the log-odds or logit of $y/x$. Indeed, this is the reason why Logistic regression receives its name.

As previously mentioned, however, Logistic regression is rather used for classification tasks. 
For a sample $x$, knowing $\hat{P}(y/x)$  the classification rule is as follows

$$\hat{y} =  
\begin{cases}
    1,& \text{if } \hat{P}(y/x) \geq 0.5\\
    0,& \text{otherwise}
\end{cases}
 $$

Since $\sigma(z) = 0.5 \iff z = 0$, it is trivial to see that we can re-write this condition as 

 $$\hat{y} =  
\begin{cases}
    1,& \text{if } w^\intercal x + b \geq 0\\
    0,& \text{otherwise}
\end{cases}
 $$

There exists a decision boundary in $w^\intercal x + b = 0$, i.e., once $w$ and $b$ are fixed, if $x$ is such that the computation of $w^\intercal x + b$ is greater than $0$, then $\hat{y} = 1$, and  $\hat{y} = 0$ otherwise. In particular, the values of $x$ that satisfy the condition $w^\intercal x + b = 0$ stand on an (affine) hyperplane in the sub-space generated by $(x_1, \ldots, x_n)$, where $b$ is the quantity by which this hyperplane is shifted from the origin. In each side of this hyperplane, $x$ is predicted to belong to a particular class.

Given a training set of $m$ samples, to obtain the optimal values for $w$ and $b$, the method of maximum likelihood is used.  The intuition behind this criteria is to choose $w$ and $b$ such that, for most samples, $\hat{P}(y =1 /x) \approx 1$ and $\hat{P}(y =1 /x) \approx 0$ when $y=1$ or $y = 0$, respectively. Without diving into the details, relying on the maximum likelihood estimation, the following cost function needs to be minimized

$$J(w, b) = \frac{1}{m}\sum_{i = 1}^m cost\Big(\hat{P}(y/x^{(i)}),  y^{(i)}\Big)$$

such that 

$$cost\Big(\hat{P}(y/x^{(i)}), y^{(i)}\Big) = 
\begin{cases}
    - log(\hat{P}(y/x^{(i)}),& \text{if } y = 1\\
    - log(1 - \hat{P}(y/x^{(i)})),& \text{otherwise}
\end{cases}
 $$
where $\hat{P}(y/x^{(i)})$ is the estimated probability that the sample $x^{(i)}$ might belong to class $y=1$ and $y^{(i)}$ is the class to which $x^{(i)}$ actually belongs to. 
