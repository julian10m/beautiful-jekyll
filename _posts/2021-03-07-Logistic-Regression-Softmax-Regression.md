---
layout: post
title: Logistic and Softmax Regression
---


Logistic and softmax regression are supervised machine learning algorithms used for classification tasks. Given a categorical target variable $y$ that may take $K$ possible values and an observation $\mathbf{x}$ composed of $n$ features/predictors, i.e., $\mathbf{x}=(x_1,\ldots,x_n)^\intercal$, these algorithms estimate the probability a posterior of $y$ given $\mathbf{x}$. On top of this estimation, these classifiers apply a decision rule that allows them to classify each class, i.e., assign them an estimated class.
## Logistic Regression

This method is used for binary classification, i.e., cases where $K=2$. However, by combining multiple logistic regression classifiers, it is possible to extend their use to multinomial cases, i.e., scenarios where $K>2$.  

### Binary Classification

In these cases the target variable can only take two possibles values, e.g., "yes/no", "green/blue", "win/lose", etc. Hence, $y$ is usually modeled as a variable that can take either 0 or  1 as values, i.e.,  $y \in \left\{0, 1\right\}$.  This way, $y=1$ may represent "yes",  "green" and "win", while $y=0$ the opposite/remaining labels. 

To classify any sample $x$, we are interested in estimating $P(y/\mathbf{x})$. Indeed,  it can be proved that, to minimize the classification error,  $\mathbf{x}$ needs to be assigned to the class that maximizes this probability.  In other words, if $P(y=1/\mathbf{x}) > P(y=0/\mathbf{x})$, our guess $\hat{y}$ for $\mathbf{x}$ would be $\hat{y} = 1$, and $\hat{y} = 0$ in the opposite case. In particular, logistic regression proposes 

$$\hat{P}(y/\mathbf{x}) = \sigma(\mathbf{w}^\intercal \mathbf{x} + b)$$

where:

- $\hat{P}(y/\mathbf{x})$ is the value estimated for $P(y/\mathbf{x})$;
 - $\mathbf{w} = (w_1, \ldots, w_n)^\intercal$ is a weighting vector of $n$ parameters;
 - $b$ is a called a bias or intercept, and;
 - $\sigma(z) = (1+e^{-z})^{-1}$ is the sigmoid function.

The sigmoid function may take any real number as input, and will always output a value between 0 and 1.  In particular,  for large negative values of $z$, the term $e^{-z}$ becomes a large positive number, and thus $\sigma(z)$ tends to $0$.  On the other hand, when $z$ takes large positive values,  $e^{-z}$ approaches $0$, and thus $\sigma(z)$ is close to $1$. For $z=0$, since $e^0 = 1$, then $\sigma(0) = 1/2$. In addition, note that

$$1 - \sigma(z) = 1 -\frac{1}{1+e^{-z}}  = \frac{e^{-z}}{1+e^{-z}}$$

In particular, this means that

$$\log\left(\frac{\sigma(z)}{1 - \sigma(z)}\right) = \log\left(\frac{1}{e^{-z}}\right) = z$$

Since $\hat{P}(y/x) = \sigma(z)~\|~z = w^\intercal x + b$, then replacing we have

$$\log\left(\frac{\hat{P}(y/\mathbf{x})}{1 - \hat{P}(y/\mathbf{x})}\right) = \mathbf{w}^\intercal \mathbf{x} + b$$

where the left term is known as the log-odds or logit of $y/\mathbf{x}$. 

The last equation resembles the one we previously saw for linear regression, where $\hat{y} =  \mathbf{w}^\intercal \mathbf{x} + b$.  This gives us a hint of from where the logistic regression name comes: comparing both expressions,
it is straightforward to conclude that logistic regression is actually like applying linear regression to the logits of $y/\mathbf{x}$.  

Despite the close relationship between linear and logistic regression, the first is used for regression tasks, but the latter in classification problems.  Indeed, logistic regressions additionally uses the following  classification rule

$$\hat{y} =  
\begin{cases}
    1,& \text{if } \hat{P}(y/x) = \sigma(\mathbf{w}^\intercal \mathbf{x} + b) \geq 0.5\\
    0,& \text{otherwise}
\end{cases}
 $$

Since $\sigma(z) = 0.5$ only when $z = 0$, it is trivial to see that we can re-write the last condition as 

 $$\hat{y} =  
\begin{cases}
    1,& \text{if } \mathbf{w}^\intercal \mathbf{x} + b \geq 0\\
    0,& \text{otherwise}
\end{cases}
 $$

Analyzing the previous expression, we can see that there exists a decision boundary in $\mathbf{w}^\intercal \mathbf{x} + b = 0$, i.e., once $\mathbf{w}$ and $b$ are fixed, if $x$ is such that the computation of $\mathbf{w}^\intercal \mathbf{x} + b$ is greater than $0$, then $\hat{y} = 1$, and  $\hat{y} = 0$ otherwise. In particular, the values of $x$ that satisfy the condition $\mathbf{w}^\intercal \mathbf{x} + b = 0$ stand on an (affine) hyperplane in the sub-space generated by $\left\{x_1, \ldots, x_n\right\}$, where $b$ is the quantity by which this hyperplane is shifted from the origin. For a sample $x$ we want to classify, $\hat{y} = 1$ if $\mathbf{x}$ falls on a specific side of this hyperplane, and $\hat{y} = 0$ in case it falls on the opposite side.

The optimal values for $\mathbf{w}$ and $b$ can be estimated relying on the maximum likelihood estimation method. Given a training set $X$ of $m$ labelled samples, modelling $y$ as a Bernoulli variable such that $p= p(\mathbf{x}, \mathbf{w}, b)$, then the joint distribution for the dataset $P\left(y^{(1)}, \ldots, y^{(m)} \Bigm\vert X\right)$ equals

$$\prod_{i=1}^m P(y^{(i)}\;\vert\;\ \mathbf{x}^{(i)}) = \prod_{i=1}^m p(\mathbf{x}^{(i)}, \mathbf{w}, b))^{y^{(i)}} \left(1 -p(\mathbf{x}^{(i)}, \mathbf{w}, b)\right)^{(1-y^{(i)})}$$

Hence the log-likelihood $l(X, \mathbf{w}, b)$ equals

$$\sum_{i=1}^m \left(y^{(i)} \log p(\mathbf{x}^{(i)}, \mathbf{w}, b) + (1 - y^{(i)})\log\left(1 -p(\mathbf{x}^{(i)}, \mathbf{w}, b)\right)\right)$$

Recalling we have modelled $p(\mathbf{x}, \mathbf{w}, b) = \hat{P}(y/\mathbf{x}) = \sigma(\mathbf{w}^\intercal \mathbf{x} + b)$, and defining the cost function $J(X, \mathbf{w}, b)$ as the negative version of the log-likelihood averaged over the $m$ samples composing the dataset, then

$$\begin{align}
J(X, \mathbf{w}, b) &=  - \frac{1}{m}l(X, w, b) \\
&= dasda \\
\end{align}$$

$$J(w, b) = -\frac{1}{m}\sum_{i = 1}^m \Bigg(y^{(i)} \log\Big(\hat{P}(y/x^{(i)})\Big)+ (1 - y^{(i)}) \log\Big(1 - \hat{P}(y/x^{(i)}\Big)\Bigg)$$

where $\hat{P}(y/x^{(i)})$ is the estimated probability that the sample $x^{(i)}$ might belong to class $y=1$ and $y^{(i)}$ is the class to which $x^{(i)}$ actually belongs to. Even though this formula looks intricate, it can be interpreted as simply averaging the result of a function $cost\Big(\hat{P}(y/x^{(i)}),  y^{(i)}\Big)$ for the $m$ samples, where

$$cost\Big(\hat{P}(y/x^{(i)}), y^{(i)}\Big) = 
\begin{cases}
    - \log(\hat{P}(y/x^{(i)}),& \text{if } y = 1\\
    - \log(1 - \hat{P}(y/x^{(i)})),& \text{otherwise}
\end{cases}
 $$

The intuition behind the expression of $J(w, b)$ is that we want to penalize the values of $w$ and $b$ not only for the mistakes that our classifier would introduce relying on them, but more generally for making doubtful classifications. In other words, for all samples $x^{(i)}$ and $x^{(j)}$ for which $y^{(i)}=1$ and $y^{(j)}=0$, we would want our classifier not to hesitate, i.e.,  to end up computing $\hat{P}(y =1 /x^{(i)}) = 1$ and $\hat{P}(y =0 /x^{(j)}) = 1$, respectively. To try to force this, what we do is to increase $J(w, b)$ when we find samples where the classifier was not completely convinced of its choice. For $y^{(i)}$ and $y^{(j)}$ we increase $J(w, b)$ in an amount $\log\big( \hat{P}(y =1 /x^{(i)})\big)$ and $\log\big(1 - \hat{P}(y =1 /x^{(j)})\big) = \log\big(\hat{P}(y =0 /x^{(j)})\big)$. Another possibility would have been to increase in $J(w, b)$ proportionally to the distance between the ideal and estimated values, i.e., $1 - \hat{P}(y =1 /x^{(i)})$ and $1 -\hat{P}(y =0 /x^{(j)})$. However, the log-scale used in logistic regression allows to penalize more worse decisions, e.g., for $y^{(i)} = 1$, then $\log\big(\hat{P}(y =1 /x^{(i)})\big)$ will be proportionally much larger the smaller $\hat{P}(y =1 /x^{(i)})$ is. During the training, the values of $w$ and $b$ will be iteratively updated, hoping that at the end of the process, the hyperplane separating zeros from ones will be reasonably well located, i.e.,  for most training samples, the estimations and classifications the algorithm will be good enough.

###  Multinomial Classification

When $K>2$, multiple logistic regression classifiers need to be used. In particular, two different approaches exist to combine them: one-vs-one (OvO) or one-vs-the-rest (OvR), also called one-vs-all (OvA).




<!-- = -\frac{1}{m}\sum_{i = 1}^m \left(y^{(i)} \log\Big(\hat{P}(y/x^{(i)})\Big)+ (1 - y^{(i)}) \log\Big(1 - \hat{P}(y/x^{(i)}\Big)\Bigg)$$
, without diving into the details, the following cost function needs to be minimized relying on the gradient descent algorithm -->