---
layout: post
title: Logistic and Softmax Regression
---

Logistic and softmax regression are supervised machine learning algorithms used for classification tasks. Given a categorical target variable $y$ that may take $K$ possible values, and an observation $\mathbf{x}$ composed of $n$ features/predictors, i.e., $\mathbf{x}=(x_1,\ldots,x_n)^\intercal$, these algorithms estimate the probability a posterior of $y$ given $\mathbf{x}$. Based on this estimation, these classifiers apply a decision rule that allows them to classify each sample, i.e., assign them an estimated class.
## Logistic Regression

This method is used for binary classification, i.e., cases where $K=2$. However, by combining multiple logistic regression classifiers, it is possible to extend their use to multinomial cases, i.e., scenarios where $K>2$.  

### Binary Classification

In these cases the target variable can only take two possibles values, e.g., "yes/no", "fun/boring", "win/lose", etc. Hence, $y$ is usually modeled as a variable that can take either 0 or  1 as values, i.e.,  $y \in \left\\{0, 1\right\\}$.  This way, $y=1$ may represent "yes",  "fun" and "win", while $y=0$ be associated to "no", "boring" and "lose". The labels may also be assigned in the opposite order, this actually has no impact on the performance of the algorithm. However, as a rule of thumb, usually when the target variable expresses the presence or absence of a property, usually $y=1$ flags the occurrence.

To classify any sample $\mathbf{x}$, we assume a probabilistic model, and thus we are interested in estimating $P(y = 1 \;\vert\; \mathbf{x})$. Indeed,  it can be proved that, to minimize the classification error,  $\mathbf{x}$ needs to be assigned to the class that maximizes the a posteriori probability.  In other words, if $P(y=1 \;\vert\; \mathbf{x}) > P(y=0 \;\vert\; \mathbf{x})$, our guess would be $\hat{y} = 1$, and $\hat{y} = 0$ otherwise. In particular, logistic regression proposes 

$$\hat{P}\left(y = 1 \;\vert\; \mathbf{x}\right) = \sigma\left(\mathbf{w}^\intercal \mathbf{x} + b\right)$$

where:

- $\hat{P}\left(y = 1 \;\vert\; \mathbf{x}\right)$ is the value estimated for $P\left(y = 1 \;\vert\; \mathbf{x}\right)$;
 - $\mathbf{w} = (w_1, \ldots, w_n)^\intercal$ is a weighting vector of $n$ parameters;
 - $b$ is a called a bias or intercept, and;
 - $\sigma(z) = \left(1+e^{-z}\right)^{-1}$ is the sigmoid function.

The sigmoid function may take any real number as input, and will always output a value between 0 and 1.  In particular,  for large negative values of $z$, the term $e^{-z}$ becomes a large positive number, and thus $\sigma(z)$ tends to $0$.  On the other hand, when $z$ takes large positive values,  $e^{-z}$ approaches $0$, and thus $\sigma(z)$ is close to $1$. For $z=0$, since $e^0 = 1$, then $\sigma(0) = 1/2$. In addition, note that

$$1 - \sigma(z) = 1 -\frac{1}{1+e^{-z}}  = \frac{e^{-z}}{1+e^{-z}}$$

In particular, this means that

$$\log\left(\frac{\sigma(z)}{1 - \sigma(z)}\right) = \log\left(\frac{1}{e^{-z}}\right) = z$$

Since $\hat{P}(y  = 1 \;\vert\; \mathbf{x}) = \sigma(z)~\|~z = \mathbf{w}^\intercal \mathbf{x} + b$, then replacing we have

$$\log\left(\frac{\hat{P}(y = 1 \;\vert\; \mathbf{x})}{1 - \hat{P}(y = 1 \;\vert\; \mathbf{x})}\right) = \mathbf{w}^\intercal \mathbf{x} + b$$

where the left term is known as the log-odds or logit of $y \;\vert\; \mathbf{x}$. 

The last equation resembles the one we previously saw for linear regression, where $\hat{y} =  \mathbf{w}^\intercal \mathbf{x} + b$.  This gives us a hint of from where the logistic regression name comes: comparing both expressions, it is straightforward to conclude that logistic regression is actually like applying linear regression to the logits of $y \;\vert\; \mathbf{x}$.  

Despite the close relationship between linear and logistic regression, the first is used for regression tasks, but the latter in classification problems.  Indeed, logistic regression additionally uses the following  classification rule

$$\hat{y} =  
\begin{cases}
    1,& \text{if } \hat{P}\left(y = 1\;\vert\; \mathbf{x}\right) \geq 0.5\\
    0,& \text{otherwise}
\end{cases}
 $$

Since $\sigma(z) = 0.5$ only when $z = 0$, it is trivial to see that we can re-write the last condition as 

 $$\hat{y} =  
\begin{cases}
    1,& \text{if } \mathbf{w}^\intercal \mathbf{x} + b \geq 0\\
    0,& \text{otherwise}
\end{cases}
 $$

Analyzing this expression, we can see that there exists a decision boundary in $\mathbf{w}^\intercal \mathbf{x} + b = 0$. The values of $\mathbf{x}$ that satisfy this condition form an (affine) hyperplane in the sub-space generated by $\left\\{x_1, \ldots, x_n\right\\}$, where $b$ is the quantity by which this hyperplane is shifted from the origin.  When $\mathbf{w}$ and $b$ are known, if $\mathbf{x}$ is such that the computation of $\mathbf{w}^\intercal \mathbf{x} + b$ is greater than $0$, then $\hat{y} = 1$, and  $\hat{y} = 0$ otherwise.

The optimal values for $\mathbf{w}$ and $b$ can be estimated relying on the maximum likelihood estimation method. Given a training set $X$ of $m$ labelled samples, i.e. $X = \left\\{(\mathbf{x}^{(1)}, y^{(1)}) \ldots, (\mathbf{x}^{(m)}, y^{(m)})\right\\}$ , modelling $y$ as a Bernoulli variable such that $p= p(\mathbf{x}, \mathbf{w}, b)$, then the joint distribution for the dataset is

$$\begin{align}
P\left(y^{(1)}, \ldots, y^{(m)} \Bigm\vert x^{(1)}, \ldots, x^{(m)}\right) &= \prod_{i=1}^m P\left(y^{(i)}\;\vert\;\ \mathbf{x}^{(i)}\right) \\
&= \prod_{i=1}^m p(\mathbf{x}^{(i)}, \mathbf{w}, b))^{y^{(i)}} \left(1 -p(\mathbf{x}^{(i)}, \mathbf{w}, b)\right)^{(1-y^{(i)})}
\end{align}
$$

where $p(\mathbf{x}^{(i)}, \mathbf{w}, b))$ is the estimated probability that sample $\mathbf{x}^{(i)}$ might belong to class $y=1$ and $y^{(i)}$ is the class to which $\mathbf{x}^{(i)}$ actually belongs to.

The log-likelihood, that we seek to maximize, can then be written as

$$l(X, \mathbf{w}, b) = \sum_{i=1}^m \left(y^{(i)} \log p(\mathbf{x}^{(i)}, \mathbf{w}, b) + (1 - y^{(i)})\log\left(1 -p(\mathbf{x}^{(i)}, \mathbf{w}, b)\right)\right)$$

Recalling we have modelled $p(\mathbf{x}, \mathbf{w}, b) = \hat{P}(y = 1 \;\vert\; \mathbf{x}) = \sigma(\mathbf{w}^\intercal \mathbf{x} + b)$, and defining the cost function $J(X, \mathbf{w}, b)$ as the negative version of the log-likelihood averaged over the $m$ samples composing the dataset, then

$$\begin{align}
J(X, \mathbf{w}, b) &=  - \frac{1}{m}l(X, w, b) \\
& = -\frac{1}{m}\sum_{i = 1}^m \left(y^{(i)} \log\left(\hat{P}\left(y = 1 \;\vert\; \mathbf{x}^{(i)}\right)\right)+ (1 - y^{(i)}) \log\left(1 - \hat{P}\left(y \;\vert\; \mathbf{x}^{(i)}\right)\right)\right)
\end{align}
$$

Since this mathematical expression resembles the one used in information theory to obtain the "cross-entropy" between two probability distributions, then this cost function is usually referred to as the **cross-entropy loss function**.  A property of this function that makes it particularly attractive is that it is a convex function, meaning that it has a minimum, and this minimum is unique.

Despite the cost function $J(X, \mathbf{w}, b)$ looks intricate, by further developing its expression, we can find a very intuitive to understand it. For this, we need to note that, for any index $i$, then $y^{(i)}$ can only evaluate to $1$ or $0$. Therefore, for each sample $(x^{(i)}, y^{(i)})$,  only one term out of the two possible contributes to the cost function. We can use this fact to compress the expression of $J(X, \mathbf{w}, b)$  as follows

$$J(X, \mathbf{w}, b) = \sum_{i=1}^m c\left(y^{(i)}, \hat{P}\left(y = 1 \;\vert\; \mathbf{x}^{(i)}\right)\right)$$

where 

$$c\left( y^{(i)}, \hat{P}(y = 1 \;\vert\; \mathbf{x}^{(i)})\right) = 
\begin{cases}
    - \log\left(\hat{P}\left(y = 1  \;\vert\;  \mathbf{x}^{(i)}\right)\right),& \text{if } y^{(i)} = 1\\
    - \log\left(1 - \hat{P}\left(y = 1 \;\vert\;  \mathbf{x}^{(i)}\right)\right),& \text{otherwise}
\end{cases}
 $$

For example, when $y^{(i)} = 1$, then if $\hat{P}\left(y = 1  \;\vert\;  \mathbf{x}^{(i)}\right) = 1$, the cost contributed is 0. However, on the opposite extreme, if $\hat{P}\left(y = 1  \;\vert\;  \mathbf{x}^{(i)}\right) = 0$, then the cost would become infinite. For intermediate values, the logarithm in the expression leads to penalize more worse decisions, e.g., for $y^{(i)} = 1$, the smaller $\hat{P}(y =1 \;\vert\; \mathbf{x}^{(i)})$ is, the much proportionally larger $\log\left(\hat{P}\left(y =1 \;\vert\; \mathbf{x}^{(i)}\right)\right)$ will be.  In addition, analyzing the expression, we see that cost when $y^{(i)} = 1$ and when $y^{(i)} = 0$ are symmetrical, i.e., the same happens when $y^{(i)} = 0$, but rather penalizing more severely  the more that $\hat{P}(y =1 \;\vert\; \mathbf{x}^{(i)})$ is closer to $1$.

Finally, it must be noted that, even if our classifier would have made correct classifications, i.e. estimated $\hat{P}(y =1 \;\vert\; \mathbf{x}^{(i)}) > 0.5$ in a case where $y^{(i)} = 1$, we are still increasing the cost function. In other words, the cost function does not penalize mistakes, but more generally the fact that the classifier makes "doubtful" classifications. In other words, for all samples $\mathbf{x}^{(i)}$ and $\mathbf{x}^{(j)}$ for which $y^{(i)}=1$ and $y^{(j)}=0$, we would want our classifier not to hesitate, i.e.,  to end up computing $\hat{P}(y = 1 \;\vert\; \mathbf{x}^{(i)}) = 1$ and $\hat{P}(y =0 \;\vert\; \mathbf{x}^{(j)}) = 1$, respectively. The cross-entropy loss function tries to force this, always increasing $J(X, \mathbf{w}, b)$ when we find samples where the classifier was not completely "convinced" or "sure" of its choice.

The optimal values of $\mathbf{w}$ and $b$ can be found minimizing the cost function $J(X, \mathbf{w}, b)$.  In particular, $J(X, \mathbf{w}, b)$ is differentiable, hence we can try to find its minimum looking for the point where all the partial derivatives become null. It is simple to show that

$$\begin{align}
\frac{\partial J(X, \mathbf{w}, b)}{\partial b} &= \frac{1}{m} \sum_{i=1}^m \left( \hat{P}\left(y = 1  \;\vert\;  \mathbf{x}^{(i)}\right) - y^{(i)}\right) \\
\frac{\partial J(X, \mathbf{w}, b)}{\partial w_j} &= \frac{1}{m} \sum_{i=1}^m x_j^{(i)}\left( \hat{P}\left(y = 1  \;\vert\;  \mathbf{x}^{(i)}\right) - y^{(i)}\right)
\end{align}
$$

Unfortunately, setting this equations to zero results into what are called transcendental equations, i.e., equations for which there is no closed form to solve them.  To overcome this limitation, the only option left is to rely on numerical solutions, such as that provided by the gradient descent algorithm.  Note that since $J(X, \mathbf{w}, b)$ is convex, converging towards the only, and thus optimal, minimum is ensured. During the training, the values of $\mathbf{w}$ and $b$ will be iteratively updated, hence modifying the positioning of the hyperplane that serves as decision boundary. Hopefully, at the end of the process, the hyperplane will end up well located, i.e., for most training samples, the estimations and classifications the algorithm performs will be reasonably good. 

To implement the gradient descent algorithm, it is convenient to have a vectorized version of the functions we have studied. Defining 

$$X^\intercal = \begin{bmatrix}
1 & 1 & \cdots & 1 \\
\mid & \mid & & \mid \\
\mathbf{x}^{(1)} & \mathbf{x}^{(2)} & \cdots & \mathbf{x}^{(m)}\\
\mid  & \mid & & \mid \\
\end{bmatrix}\quad
\Theta  = \begin{bmatrix}
b \\
\mathbf{w} \\
\end{bmatrix}\quad
\mathbf{y}  = \begin{bmatrix}
y^{(1)} \\
y^{(2)} \\
\vdots \\
y^{(m)} \\
\end{bmatrix}
$$

we can then write 

$$J(X, \Theta) = -\frac{1}{m}\Big(\mathbf{y}^\intercal \log\left(\sigma(X \Theta)\right)+ (1 - \mathbf{y}^\intercal) \log\left(1 - \sigma(X \Theta)\right)\Big)$$

$$\nabla J(X, \Theta) = \frac{1}{m} X^\intercal\left(\sigma(X \Theta) - \mathbf{y}\right)$$

###  Multinomial Classification

When $K>2$, multiple logistic regression classifiers need to be used. In particular, two different approaches exist to combine them: one-vs-one (OvO) or one-vs-the-rest (OvR), also called one-vs-all (OvA).
